Leveraging Historical Market Data for Stock Price Prediction using RNN-Based Model (LSTM)

Project for: Abdulrashid Abubakar (DL/CSC/21D/0025)
Institution: Modibbo Adama University of Technology, Yola (MAU, YOLA)

This notebook implements the core requirements of your project: data consolidation, preprocessing, building an LSTM-based prediction model, and preparing the necessary code and files for local deployment using Streamlit.

1. Setup, Data Loading, and Merging

We start by installing all required libraries and then load, merge, and organize the raw CSV files into a single, comprehensive DataFrame.

1.1 Install Necessary Libraries

# Install required libraries
!pip install pandas numpy tensorflow scikit-learn streamlit joblib --quiet
# joblib is required for saving/loading the scaler


1.2 Data Merging Logic

CRITICAL STEP: Ensure all 10 CSV files are uploaded to your Colab session's file system before running this block.

import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
import joblib

# List of files and their corresponding organization names (first word of the file name)
file_org_map = {
    "AIICO Historical Data.csv": "AIICO",
    "DANGCEM Historical Data.csv": "DANGCEM",
    "GUINNES Historical Data.csv": "GUINNES",
    "JBERGER Historical Data.csv": "JBERGER",
    "NB Historical Data.csv": "NB",
    "NESTLE Historical Data.csv": "NESTLE",
    "NSE All Share Historical Data (1).csv": "NSE",
    "UBA Historical Data.csv": "UBA",
    "UNILEVE Historical Data.csv": "UNILEVE",
    "ZENITHB Historical Data.csv": "ZENITHB"
}

all_data = []

print("Starting data merging. Please ensure all 10 CSV files are uploaded to Colab's file system.")

for file_name, organisation_name in file_org_map.items():
    try:
        # Load files, assuming core columns are present
        df = pd.read_csv(file_name, low_memory=False)
        
        # Keep only the core columns and add Organisation
        df = df[['Date', 'Price', 'Open', 'High', 'Low', 'Vol.', 'Change %']]
        df['Organisation'] = organisation_name
        
        all_data.append(df)
    except FileNotFoundError:
        print(f"Error: {file_name} not found. Skipping.")
    except Exception as e:
        print(f"Error processing {file_name}: {e}. Skipping.")

merged_df = pd.concat(all_data, ignore_index=True)
print(f"\nTotal merged records: {len(merged_df)}")
print("\nSample Merged Data Head:")
print(merged_df.head())


2. Data Cleaning and Preprocessing

We clean the data by converting price columns (which may contain commas), Volume (with 'M'/'K' suffixes), and Change % (with '%') into numerical floats, which is essential for machine learning.

def clean_numeric_column(series):
    """
    Cleans stock data columns by removing commas, converting 'M'/'K' suffixes 
    to numeric values, and removing '%' signs.
    """
    series = series.astype(str).str.replace(',', '', regex=False)
    
    def convert_value(x):
        if pd.isna(x) or x.strip() == '':
            return np.nan
        x = x.strip().upper()
        
        # Handle Volume suffixes
        if 'M' in x:
            return float(x.replace('M', '')) * 1_000_000
        elif 'K' in x:
            return float(x.replace('K', '')) * 1_000
            
        # Handle Change %
        elif '%' in x:
            # Convert percentage string to a float ratio (e.g., '1.53%' -> 0.0153)
            return float(x.replace('%', '')) / 100.0
            
        try:
            return float(x)
        except ValueError:
            return np.nan

    return series.apply(convert_value)

# Apply cleaning to all relevant columns
merged_df['Vol.'] = clean_numeric_column(merged_df['Vol.'])
merged_df['Change %'] = clean_numeric_column(merged_df['Change %'])

for col in ['Price', 'Open', 'High', 'Low']:
    merged_df[col] = clean_numeric_column(merged_df[col])

# Convert Date column to datetime format
merged_df['Date'] = pd.to_datetime(merged_df['Date'], infer_datetime_format=True, errors='coerce')

# Drop rows with any NaN values in critical columns
critical_cols = ['Date', 'Price', 'Vol.', 'Change %']
merged_df.dropna(subset=critical_cols, inplace=True)

# Sort by Organisation and Date (essential for time series)
merged_df.sort_values(by=['Organisation', 'Date'], inplace=True)

print("\nSample Cleaned Data (Numeric Columns and Date):")
print(merged_df.head())
print(merged_df.info())


3. LSTM Model Development (Focusing on NSE All Share Index)

The LSTM model is trained on the NSE All Share Index (NSE) closing price ('Price'). This approach models the general Nigerian market trend.

3.1 Isolate and Scale Data

# --- 1. Isolate Target Stock Data (NSE) ---
df_nse = merged_df[merged_df['Organisation'] == 'NSE'].copy()
df_nse = df_nse.sort_values('Date')

# Use only the 'Price' (Closing Price) column for forecasting
data = df_nse['Price'].values.reshape(-1, 1)

# --- 2. Scaling ---
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# --- 3. Create Training Data Structure ---
# LSTMs use a 'lookback' period. Past 60 prices predict the next day's price.
LOOKBACK_PERIOD = 60
X_train = []
y_train = []

for i in range(LOOKBACK_PERIOD, len(scaled_data)):
    # X_train: last 60 prices (lookback sequence)
    X_train.append(scaled_data[i-LOOKBACK_PERIOD:i, 0])
    # y_train: the 61st price (the target)
    y_train.append(scaled_data[i, 0])

X_train, y_train = np.array(X_train), np.array(y_train)

# Reshape X_train for LSTM input: [samples, time_steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

print(f"\nTraining data shape: {X_train.shape}")
print(f"Target data shape: {y_train.shape}")


3.2 Build and Train the LSTM Model

# --- 4. Build the LSTM Model ---
model = Sequential()

# First LSTM Layer: return_sequences=True for stacking LSTMs
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(Dropout(0.2)) 

# Second LSTM Layer: return_sequences=False for final output to Dense layer
model.add(LSTM(units=50, return_sequences=False)) 
model.add(Dropout(0.2))

# Output Layer: One unit for the single price prediction
model.add(Dense(units=1)) 

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# --- 5. Train the Model ---
print("\nStarting model training...")
# Note: For your final project, consider increasing epochs (e.g., 25-50) for better accuracy.
history = model.fit(
    X_train, 
    y_train, 
    epochs=10, 
    batch_size=32,
    validation_split=0.1, 
    verbose=1
)

print("\nModel training complete.")
print(model.summary())


3.3 Save Model and Data Files (For Streamlit Deployment)

This step saves the necessary components to your Colab session's disk. You must download these three files to your local computer for the Streamlit deployment to work.

# --- 6. Save the Trained Model, Scaler, and Cleaned Data ---
MODEL_FILE = 'lstm_model_nse.h5'
SCALER_FILE = 'scaler_nse.joblib'
DATA_FILE = 'cleaned_nigerian_stock_data.csv'

# Save the Keras model
model.save(MODEL_FILE)
print(f"Model saved as: {MODEL_FILE}")

# Save the scaler object
joblib.dump(scaler, SCALER_FILE)
print(f"Scaler saved as: {SCALER_FILE}")

# Save the cleaned merged data
merged_df.to_csv(DATA_FILE, index=False)
print(f"Cleaned data saved as: {DATA_FILE}")

# Download instruction for the user
print("\n--- ACTION REQUIRED ---")
print("1. Download the following three files from the Colab file explorer (right-click -> Download):")
print(f"   - {MODEL_FILE}")
print(f"   - {SCALER_FILE}")
print(f"   - {DATA_FILE}")
print("2. Save the Streamlit script (`app.py` in Section 4) in the same local folder as these files.")
print("3. Run the Streamlit app locally using: streamlit run app.py")


4. Streamlit Deployment Code (app.py)

Copy the entire block below and save it as a separate file named app.py in the same local directory where you downloaded the three files from Step 3.3.

# --- COPY THIS BLOCK INTO A FILE NAMED 'app.py' LOCALLY ---
import streamlit as st
import pandas as pd
import numpy as np
import joblib
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt

# --- 1. CONFIGURATION AND LOADING ---

# Define file paths
MODEL_FILE = 'lstm_model_nse.h5'
SCALER_FILE = 'scaler_nse.joblib'
DATA_FILE = 'cleaned_nigerian_stock_data.csv'
LOOKBACK_PERIOD = 60 # Must match the training lookback period

st.set_page_config(layout="wide", page_title="Stock Price Prediction (LSTM)")

# Cache resources (model and scaler) so they only load once
@st.cache_resource
def load_all_files():
    """Loads model, scaler, and cleaned data from disk."""
    try:
        model = load_model(MODEL_FILE)
        scaler = joblib.load(SCALER_FILE)
        df = pd.read_csv(DATA_FILE)
        df['Date'] = pd.to_datetime(df['Date'])
        return model, scaler, df
    except FileNotFoundError as e:
        st.error(f"FATAL ERROR: Required file not found. Please ensure {e.filename} and the other two files are in the same folder as app.py.")
        return None, None, pd.DataFrame()

model, scaler, df_all = load_all_files()

# --- 2. APP LAYOUT AND CHECKS ---

st.title("ðŸ‡³ðŸ‡¬ NSE Stock Price Forecasting (LSTM RNN)")
st.caption("Developed by Abdulrashid Abubakar | Modibbo Adama University, Yola")

if df_all.empty or model is None or scaler is None:
    st.stop()

# Sidebar for selection
organizations = df_all['Organisation'].unique()
# Default to NSE index, as that is what the model was trained on
default_index = np.where(organizations == 'NSE')[0][0] if 'NSE' in organizations else 0 
selected_org = st.sidebar.selectbox("Select Organisation/Ticker for Analysis:", organizations, index=default_index) 

st.sidebar.markdown("---")
st.sidebar.subheader("Model Information")
st.sidebar.info(f"Model trained on the **NSE All Share Index** using a **{LOOKBACK_PERIOD}-day** lookback window. It is applied to the selected stock.")

# Filter data for the selected organization
df_org = df_all[df_all['Organisation'] == selected_org].sort_values('Date').copy()


# --- 3. PREDICTION FUNCTION ---

def predict_next_day(org_data, model, scaler):
    """Generates the prediction for the next trading day."""
    
    if len(org_data) < LOOKBACK_PERIOD:
        return None, None, None # Not enough data
        
    # Use the 'Price' column for prediction
    last_prices = org_data['Price'].values[-LOOKBACK_PERIOD:].reshape(-1, 1)
    
    # 1. Scale the input data using the trained scaler
    scaled_input = scaler.transform(last_prices)
    
    # 2. Reshape for LSTM input: [1, 60, 1]
    X_test = np.reshape(scaled_input, (1, LOOKBACK_PERIOD, 1))
    
    # 3. Predict the scaled price
    scaled_prediction = model.predict(X_test, verbose=0)
    
    # 4. Inverse transform to get the actual Naira price
    predicted_price = scaler.inverse_transform(scaled_prediction)[0, 0]
    
    # Calculate the next trading date
    last_date = org_data['Date'].max()
    next_date = last_date + pd.Timedelta(days=1)
    
    # Advance to the next weekday (simple trading day approximation)
    while next_date.weekday() > 4: # 5=Saturday, 6=Sunday
        next_date += pd.Timedelta(days=1)
        
    return predicted_price, last_date, next_date

# --- 4. MAIN METRICS DISPLAY ---

if not df_org.empty:
    latest_price = df_org['Price'].iloc[-1]
    latest_date = df_org['Date'].iloc[-1].strftime('%Y-%m-%d')
    mean_volume = df_org['Vol.'].mean() / 1_000_000 # Convert to millions

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric(f"Latest Close Price ({latest_date})", f"â‚¦{latest_price:,.2f}")
    with col2:
        st.metric("Average Volume (Millions)", f"{mean_volume:,.2f}M")
    with col3:
        st.metric("Total Data Points", f"{len(df_org):,}")
else:
    st.stop()


# --- 5. PREDICTION SECTION ---
st.header(f"Forecast for {selected_org}")

if st.button(f"Predict Next Trading Day Price for {selected_org}"):
    predicted_price, last_date, next_date = predict_next_day(df_org, model, scaler)
    
    if predicted_price is not None:
        st.success(f"**Predicted Closing Price for {selected_org} on {next_date.strftime('%Y-%m-%d')}:**")
        st.balloons()
        
        # Display the main prediction
        st.markdown(f"## â‚¦{predicted_price:,.2f}")
        
        # Comparison to latest price
        change_pct = (predicted_price - latest_price) / latest_price * 100
        st.markdown(f"*(Change from previous close: **{change_pct:+.2f}%**)*")
        
        if change_pct > 0:
            st.markdown("**(Predicted Trend: UP)**")
        elif change_pct < 0:
            st.markdown("**(Predicted Trend: DOWN)**")
        else:
            st.markdown("**(Predicted Trend: NEUTRAL)**")
            
    else:
        st.warning(f"Not enough historical data for {selected_org} (Requires at least {LOOKBACK_PERIOD} days).")


# --- 6. VISUALIZATION ---
st.header("Historical Price Trend")

# Create a figure for plotting
fig, ax = plt.subplots(figsize=(10, 5))

# Plot the historical closing price
ax.plot(df_org['Date'], df_org['Price'], label='Actual Closing Price', color='#007A33', linewidth=2) # NSE Green
ax.set_title(f'Historical Closing Price for {selected_org}', fontsize=16)
ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Price (â‚¦)', fontsize=12)
ax.legend()
ax.grid(True, linestyle=':', alpha=0.7)
plt.xticks(rotation=45)
plt.tight_layout()

st.pyplot(fig)
